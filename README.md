# ImageProcessing-and-TextScraping
Slicing images and scraping text with pytesseract

I came across a freelance job posting for data entry. The job poster provided a folder full of screenshots of company names and logos from some company aggregation site, the task being to put each company into a spreadsheet, alongside additional information about the company which was to be researched.

I figured this entire process could be easily automated. Firstly, rather than taking screenshots of the website and then give them to someone to read, one could easily utilize a webscraper to scrape the company names in one go and put them into a csv or database. For the research portion, a similar method could be applied with the collected company names and automated searches. I decided to turn my attention to the images that were given in any case.

Python tesseract is a popular library for OCR, or optical character recognition. I decided to try using tesseract to scrape text from the screenshots. I realized early on I would need to process the images. The company logos, especially ones with characters or words, would be read, returning unwanted/garbled text. If more than one company names were in one image, they would be returned all at one, making it difficult to distinguish or sepearate companies without significant human oversight, which is what we're trying to avoid.

I thus turned to OpenCV's image loading and numpy array slicing to select portions of images. The images were sliced in a way such that the final image passed to tesseract had no logo and had only one company's text. With this preprocessing complete, and pytesseract loaded, everything ran perfectly and all the results were saved to a csv. 

It was then I came upon a fatal problem. While most of the scraped text was beautiful and perfectly extracted, a significant portion was split, cutoff, and otherwise obviously incorrect. It occured to me that, not only were the images different sizes, but the company names were positioned slightly differently on each image. This meant that during the automated slicing, some images were sliced in such a way that parts of text would be cut off or seperated. The cutoff words would then be read by tesseract, returning  garbage text, empty spaces, symbols, etc. Thus, for this to work, I would need to somehow standardize the position of the company tiles.

After some browsing on stackoverflow, I found a function that seems to help acheive what I want. The function (found here https://stackoverflow.com/questions/49907382/how-to-remove-whitespace-from-an-image-in-opencv) removes all the whitespace surrounding the image, seemingly standardizing the size and position of the company tiles. Running with these de-whitespaced images results in far fewer mistakes and misreads.

However, it is still not perfect. Of course, pytesseract cannot read everything perfectly (it is frequently convinced an 8 is a 6, for instance). Furthermore, the image grid splitting is flawed. Not all the company name boxes are the same size, and some indeed extend outside of their given grid tile, leading to cut off text still. I must come up with an improved way of obtaining the company name boxes. Maybe I can implement some sort of machine learning alogrithm to teach the program what to find? Or maybe just implement a function that cuts based on the thin grey line that encapsulates the company logo+name box.
